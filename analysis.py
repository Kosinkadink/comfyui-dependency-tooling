import json
import argparse
import re
import fnmatch
import os
import time
import requests
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# Import from src modules
from src.graph import create_cumulative_graph
from src.utils import make_filename_safe


base_url = "https://api.comfy.org"

def get_registry_nodes(print_time=True):
    # get all nodes from registry in a similar fashion as ComfyUI-Manager
    # this is a rare piece of code here that is not generated by Claude Code
    nodes_dict = {}

    def fetch_all(print_time=False):
        remaining = True
        full_nodes = {}
        page = 1

        start_time = time.perf_counter()
        while remaining:
            sub_uri = f'{base_url}/nodes?page={page}&limit=30'
            json_obj = requests.get(sub_uri).json()
            remaining = page < json_obj['totalPages']

            for x in json_obj['nodes']:
                full_nodes[x['id']] = x

            page += 1

        end_time = time.perf_counter()
        if print_time:
            print(f"Time taken to fetch all nodes: {end_time - start_time:.2f} seconds")

        return full_nodes

    nodes_dict = fetch_all(print_time=print_time)

    for v in nodes_dict.values():
        if 'latest_version' not in v:
            v['latest_version'] = dict(version='nightly')

    return {'nodes': list(nodes_dict.values())}



def save_nodes_json(registry_data, filepath='manager-files/nodes.json'):
    """
    Save registry data to nodes.json file.

    Args:
        registry_data: The data to save (should have 'nodes' key)
        filepath: Path to save the file to

    Returns:
        True if successful, False otherwise
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # Save with indent=4 for readability
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(registry_data, f, indent=4)

        print(f"Successfully updated {filepath}")
        print(f"Total nodes saved: {len(registry_data['nodes'])}")
        return True
    except Exception as e:
        print(f"Error saving nodes.json: {e}")
        return False


def load_nodes_to_dict(filepath='manager-files/nodes.json'):
    """
    Load nodes.json and convert to a dictionary with 'id' as keys.

    Args:
        filepath: Path to the nodes.json file

    Returns:
        Dictionary where keys are node IDs and values are the node data
    """
    nodes_dict = {}

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if 'nodes' in data:
            for node in data['nodes']:
                if 'id' in node:
                    node_id = node['id']
                    nodes_dict[node_id] = node
                else:
                    print(f"Warning: Node without 'id' field found and skipped")

        print(f"Successfully loaded {len(nodes_dict)} nodes")
        return nodes_dict

    except FileNotFoundError:
        print(f"Error: File '{filepath}' not found")
        return {}
    except json.JSONDecodeError as e:
        print(f"Error: Failed to parse JSON - {e}")
        return {}
    except Exception as e:
        print(f"Error: {e}")
        return {}


def compile_dependencies(nodes_dict):
    """
    Compile all dependencies from the latest version of each node.
    Groups all versions of the same package together for accurate statistics.
    Excludes commented dependencies (starting with #) and pip commands (starting with --).
    Tracks nodes that have commented deps or pip commands.

    Args:
        nodes_dict: Dictionary of nodes with IDs as keys

    Returns:
        Dictionary with dependency statistics and lists
    """
    all_dependencies_raw = []  # Keep raw versions for backward compatibility
    base_dependency_count = defaultdict(int)  # Count by base package name
    dependency_versions = defaultdict(set)  # Track all version specs per base package
    nodes_with_deps = []
    nodes_without_deps = []
    nodes_with_commented_deps = []
    nodes_with_pip_commands = []
    nodes_with_git_deps = []  # Track nodes with git-based dependencies
    commented_dependencies = []
    pip_commands = []
    pip_command_count = defaultdict(int)
    git_dependencies = []  # All git-based dependencies
    git_dependency_count = defaultdict(int)  # Count by type of git dep

    for node_id, node_data in nodes_dict.items():
        if 'latest_version' in node_data and node_data['latest_version']:
            latest_version = node_data['latest_version']

            if 'dependencies' in latest_version:
                deps = latest_version['dependencies']

                if deps and isinstance(deps, list) and len(deps) > 0:
                    active_deps = []
                    commented_deps = []
                    node_pip_commands = []
                    node_git_deps = []

                    for dep in deps:
                        dep_str = str(dep).strip()

                        # Skip lines that start with # (full line comments)
                        if dep_str.startswith('#'):
                            commented_deps.append(dep_str)
                            commented_dependencies.append(dep_str)
                            continue

                        # Check for pip commands (starting with --)
                        if dep_str.startswith('--'):
                            node_pip_commands.append(dep_str)
                            pip_commands.append(dep_str)
                            pip_command_count[dep_str] += 1
                        else:
                            # Strip inline comments (anything after #)
                            if '#' in dep_str:
                                dep_str = dep_str.split('#')[0].strip()

                            # Skip if the line becomes empty after stripping comments
                            if not dep_str:
                                continue

                            # Check for git-based dependencies
                            is_git_dep = False

                            # Type 1: Dependencies starting with git+
                            if dep_str.startswith('git+'):
                                is_git_dep = True
                                node_git_deps.append(dep_str)
                                git_dependencies.append(dep_str)
                                git_dependency_count['git+ prefix'] += 1
                                # Extract just the URL part for base name
                                base_name = dep_str

                            # Type 2: Dependencies with @ git+ (e.g., package @ git+https://...)
                            elif ' @ git+' in dep_str:
                                is_git_dep = True
                                node_git_deps.append(dep_str)
                                git_dependencies.append(dep_str)
                                git_dependency_count['@ git+ style'] += 1
                                # Extract package name before @
                                base_name = dep_str.split(' @ ')[0].strip().lower()

                            # Regular dependency
                            else:
                                # Extract base package name (before version specifiers)
                                dep_lower = dep_str.lower()
                                base_name = re.split(r'[<>=!~]', dep_lower)[0].strip()

                            # Add to active dependencies
                            active_deps.append(dep_str)
                            all_dependencies_raw.append(dep_str)

                            # Count by base name
                            base_dependency_count[base_name] += 1

                            # Track the full version spec
                            dependency_versions[base_name].add(dep_str)

                    if node_pip_commands:
                        nodes_with_pip_commands.append({
                            'id': node_id,
                            'name': node_data.get('name', 'N/A'),
                            'pip_commands': node_pip_commands,
                            'active_deps': active_deps
                        })

                    if commented_deps:
                        nodes_with_commented_deps.append({
                            'id': node_id,
                            'name': node_data.get('name', 'N/A'),
                            'commented_deps': commented_deps,
                            'active_deps': active_deps
                        })

                    if node_git_deps:
                        nodes_with_git_deps.append({
                            'id': node_id,
                            'name': node_data.get('name', 'N/A'),
                            'git_deps': node_git_deps,
                            'active_deps': active_deps
                        })

                    if active_deps:
                        nodes_with_deps.append({
                            'id': node_id,
                            'name': node_data.get('name', 'N/A'),
                            'dependencies': active_deps
                        })
                    else:
                        # Node has only commented dependencies
                        nodes_without_deps.append(node_id)
                else:
                    nodes_without_deps.append(node_id)
            else:
                nodes_without_deps.append(node_id)
        else:
            nodes_without_deps.append(node_id)

    # Create sorted list by base package frequency
    sorted_base_dependencies = sorted(base_dependency_count.items(), key=lambda x: x[1], reverse=True)

    # Get unique base package names
    unique_base_dependencies = list(base_dependency_count.keys())

    # Keep raw unique dependencies for backward compatibility
    unique_dependencies_raw = list(set(all_dependencies_raw))
    unique_commented = list(set(commented_dependencies))

    # Sort pip commands by frequency
    sorted_pip_commands = sorted(pip_command_count.items(), key=lambda x: x[1], reverse=True)
    unique_pip_commands = list(pip_command_count.keys())

    # Process git dependencies
    unique_git_dependencies = list(set(git_dependencies))
    sorted_git_dependency_types = sorted(git_dependency_count.items(), key=lambda x: x[1], reverse=True)

    return {
        'all_dependencies': all_dependencies_raw,
        'unique_dependencies': unique_dependencies_raw,  # Raw unique deps for compatibility
        'dependency_count': base_dependency_count,  # Now counts by base package name
        'sorted_by_frequency': sorted_base_dependencies,  # Sorted by base package frequency
        'dependency_versions': dict(dependency_versions),  # All version specs per package
        'unique_base_dependencies': unique_base_dependencies,  # Unique base package names
        'nodes_with_dependencies': nodes_with_deps,
        'nodes_without_dependencies': nodes_without_deps,
        'nodes_with_commented_dependencies': nodes_with_commented_deps,
        'nodes_with_pip_commands': nodes_with_pip_commands,
        'nodes_with_git_dependencies': nodes_with_git_deps,  # Nodes with git-based deps
        'commented_dependencies': commented_dependencies,
        'unique_commented_dependencies': unique_commented,
        'pip_commands': pip_commands,
        'unique_pip_commands': unique_pip_commands,
        'pip_command_count': dict(pip_command_count),
        'sorted_pip_commands': sorted_pip_commands,
        'git_dependencies': git_dependencies,  # All git-based dependencies
        'unique_git_dependencies': unique_git_dependencies,  # Unique git deps
        'git_dependency_count': dict(git_dependency_count),  # Count by type
        'sorted_git_dependency_types': sorted_git_dependency_types,  # Sorted by count
        'total_dependencies': len(all_dependencies_raw),
        'unique_count': len(unique_base_dependencies),  # Count of unique base packages
        'unique_raw_count': len(unique_dependencies_raw),  # Count of unique raw specs
        'nodes_with_deps_count': len(nodes_with_deps),
        'nodes_without_deps_count': len(nodes_without_deps),
        'nodes_with_commented_count': len(nodes_with_commented_deps),
        'nodes_with_pip_commands_count': len(nodes_with_pip_commands),
        'nodes_with_git_deps_count': len(nodes_with_git_deps)  # Count of nodes with git deps
    }


def analyze_wildcard_dependencies(nodes_dict, pattern):
    """
    Analyze multiple dependencies matching a wildcard pattern.

    Args:
        nodes_dict: Dictionary of nodes
        pattern: Wildcard pattern (e.g., "torch*")

    Returns:
        Dictionary with info about all matching dependencies
    """
    pattern_lower = pattern.lower()
    matching_deps = {}

    # First, collect all unique base dependencies
    all_base_deps = set()
    for node_id, node_data in nodes_dict.items():
        if 'latest_version' in node_data and node_data['latest_version']:
            latest_version = node_data['latest_version']
            if 'dependencies' in latest_version and latest_version['dependencies']:
                for dep in latest_version['dependencies']:
                    dep_str = str(dep).strip()
                    # Skip full line comments and pip commands
                    if dep_str.startswith('#') or dep_str.startswith('--'):
                        continue

                    # Strip inline comments
                    if '#' in dep_str:
                        dep_str = dep_str.split('#')[0].strip()

                    # Skip if empty after stripping
                    if not dep_str:
                        continue

                    dep_lower = dep_str.lower()
                    base_name = re.split(r'[<>=!~]', dep_lower)[0].strip()
                    all_base_deps.add(base_name)

    # Find dependencies matching the pattern
    for base_dep in all_base_deps:
        if fnmatch.fnmatch(base_dep, pattern_lower):
            # Analyze each matching dependency
            dep_info = analyze_specific_dependency(nodes_dict, base_dep)
            matching_deps[base_dep] = dep_info

    return matching_deps


def calculate_node_ranks(nodes_dict):
    """
    Calculate download ranks for all nodes.

    Args:
        nodes_dict: Dictionary of nodes

    Returns:
        Dictionary mapping node_id to rank (1-based)
    """
    # Sort all nodes by downloads
    sorted_nodes = sorted(nodes_dict.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)

    # Create rank mapping
    rank_map = {}
    for rank, (node_id, _) in enumerate(sorted_nodes, 1):
        rank_map[node_id] = rank

    return rank_map


def analyze_specific_dependency(nodes_dict, dep_name):
    """
    Analyze a specific dependency across all nodes.
    Excludes commented dependencies but notes if the dependency appears commented.

    Args:
        nodes_dict: Dictionary of nodes
        dep_name: Name of dependency to analyze

    Returns:
        Dictionary with detailed info about the dependency
    """
    dep_name_lower = dep_name.lower()
    nodes_using = []
    all_versions = []
    version_count = defaultdict(int)
    nodes_with_commented = []

    # Calculate ranks for all nodes
    rank_map = calculate_node_ranks(nodes_dict)

    for node_id, node_data in nodes_dict.items():
        if 'latest_version' in node_data and node_data['latest_version']:
            latest_version = node_data['latest_version']

            if 'dependencies' in latest_version and latest_version['dependencies']:
                for dep in latest_version['dependencies']:
                    dep_str = str(dep).strip()

                    # Skip full line comments
                    if dep_str.startswith('#'):
                        # Check if this commented line mentions our dependency
                        commented_content = dep_str[1:].strip()
                        commented_lower = commented_content.lower()
                        base_name = re.split(r'[<>=!~]', commented_lower)[0].strip()

                        if base_name == dep_name_lower:
                            nodes_with_commented.append({
                                'node_id': node_id,
                                'node_name': node_data.get('name', 'N/A'),
                                'commented_spec': dep_str
                            })
                        continue  # Skip commented lines for main analysis

                    # Strip inline comments (anything after #)
                    if '#' in dep_str:
                        dep_str = dep_str.split('#')[0].strip()

                    # Skip if the line becomes empty after stripping comments
                    if not dep_str:
                        continue

                    # Parse dependency string (could be just name or name with version)
                    dep_lower = dep_str.lower()

                    # Extract base name (before any version specifier)
                    base_name = re.split(r'[<>=!~]', dep_lower)[0].strip()

                    if base_name == dep_name_lower:
                        # Extract additional node information
                        latest_version_info = node_data.get('latest_version', {})
                        latest_version_date = 'N/A'
                        if latest_version_info and 'createdAt' in latest_version_info:
                            # Parse and format the date
                            date_str = latest_version_info['createdAt']
                            try:
                                # Extract just the date portion (YYYY-MM-DD)
                                latest_version_date = date_str[:10] if date_str else 'N/A'
                            except:
                                latest_version_date = 'N/A'

                        nodes_using.append({
                            'node_id': node_id,
                            'node_name': node_data.get('name', 'N/A'),
                            'repository': node_data.get('repository', 'N/A'),
                            'dependency_spec': dep_str,
                            'stars': node_data.get('github_stars', 0),
                            'downloads': node_data.get('downloads', 0),
                            'rank': rank_map.get(node_id, 0),
                            'latest_version_date': latest_version_date
                        })

                        # Extract version if present
                        version_match = re.search(r'[<>=!~]+(.+)', dep_str)
                        if version_match:
                            version_spec = dep_str[len(base_name):].strip()
                            all_versions.append(version_spec)
                            version_count[version_spec] += 1
                        else:
                            all_versions.append('*')
                            version_count['*'] += 1

    return {
        'dependency_name': dep_name,
        'nodes_using': nodes_using,
        'total_nodes': len(nodes_using),
        'all_versions': all_versions,
        'unique_versions': list(set(all_versions)),
        'version_count': dict(version_count),
        'sorted_versions': sorted(version_count.items(), key=lambda x: x[1], reverse=True),
        'nodes_with_commented': nodes_with_commented,
        'commented_count': len(nodes_with_commented)
    }


def save_results_to_file(query, results_text):
    """Save search results to a timestamped file in the results directory."""
    # Create results directory if it doesn't exist
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)

    # Generate filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    safe_query = make_filename_safe(query)
    filename = f"{timestamp}_{safe_query}.txt"
    filepath = results_dir / filename

    # Write results to file
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"Query: {query}\n")
            f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n\n")
            f.write(results_text)
        print(f"\n[Results saved to: {filepath}]")
        return True
    except Exception as e:
        print(f"\n[Error saving results: {e}]")
        return False


def format_dependency_details(result, show_all_nodes=False):
    """Format detailed dependency information for display and saving."""
    lines = []
    lines.append(f"\n{'='*50}")
    lines.append(f"Dependency: {result['dependency_name']}")
    lines.append(f"{'='*50}")
    lines.append(f"Total nodes using this dependency: {result['total_nodes']}")

    if result['commented_count'] > 0:
        lines.append(f"Nodes with commented-out lines: {result['commented_count']}")

    if result['total_nodes'] > 0:
        lines.append(f"\nVersion specifications found:")
        for version, count in result['sorted_versions']:
            lines.append(f"  {version:20} - {count} nodes")

        # Sort nodes by downloads (most popular first)
        sorted_nodes = sorted(result['nodes_using'],
                            key=lambda x: x.get('downloads', 0),
                            reverse=True)

        # Show all nodes if requested, otherwise limit to 10
        nodes_to_show = sorted_nodes if show_all_nodes else sorted_nodes[:10]

        lines.append(f"\nNodes using {result['dependency_name']} ({'all' if show_all_nodes else 'showing first 10 by popularity'}):")
        for i, node in enumerate(nodes_to_show, 1):
            lines.append(f"\n  {i}. {node['node_name']} ({node['node_id']})")
            lines.append(f"     Rank: #{node.get('rank', 'N/A')} | Downloads: {node.get('downloads', 0):,} | Stars: {node.get('stars', 0):,} | Latest: {node.get('latest_version_date', 'N/A')}")
            lines.append(f"     Spec: {node['dependency_spec']}")
            lines.append(f"     Repo: {node['repository']}")

        if not show_all_nodes and result['total_nodes'] > 10:
            lines.append(f"\n  ... and {result['total_nodes'] - 10} more nodes")

    if result['commented_count'] > 0:
        lines.append(f"\n\nNodes with commented-out {result['dependency_name']} lines:")
        for i, node in enumerate(result['nodes_with_commented'], 1):
            lines.append(f"  {i}. {node['node_name']} ({node['node_id']})")
            lines.append(f"     Commented line: {node['commented_spec']}")

    return '\n'.join(lines)


def parse_nodes_modifier(query, nodes_dict):
    """
    Parse the &nodes modifier to filter nodes by specific IDs.

    Args:
        query: Query string containing the modifier
        nodes_dict: Full dictionary of nodes

    Returns:
        Filtered nodes dictionary or original if no modifier
    """
    import re

    # Check if &nodes modifier is present
    nodes_match = re.search(r'&nodes\s+([^&]+)', query, re.IGNORECASE)
    if not nodes_match:
        return nodes_dict

    nodes_spec = nodes_match.group(1).strip()

    # Check if it's a file specification
    if nodes_spec.startswith('file:'):
        filepath = nodes_spec[5:].strip()
        try:
            with open(filepath, 'r') as f:
                node_ids = [line.strip() for line in f if line.strip()]
            print(f"\n[Loaded {len(node_ids)} node IDs from {filepath}]")
        except FileNotFoundError:
            print(f"\n[Error: File not found: {filepath}]")
            return nodes_dict
        except Exception as e:
            print(f"\n[Error reading file: {e}]")
            return nodes_dict
    else:
        # Parse comma-separated list
        node_ids = [id.strip() for id in nodes_spec.split(',') if id.strip()]
        print(f"\n[Filtering to {len(node_ids)} specific nodes]")

    # Filter nodes dictionary
    filtered_nodes = {}
    missing_ids = []

    for node_id in node_ids:
        if node_id in nodes_dict:
            filtered_nodes[node_id] = nodes_dict[node_id]
        else:
            missing_ids.append(node_id)

    if missing_ids:
        print(f"[Warning: {len(missing_ids)} node IDs not found: {', '.join(missing_ids[:5])}{'...' if len(missing_ids) > 5 else ''}]")

    return filtered_nodes if filtered_nodes else nodes_dict


def print_help():
    """Print help information for commands and modifiers."""
    print("\nCommands:")
    print("  /list  - Show all unique dependency names")
    print("         Use &dupes to show only deps with version conflicts")
    print("  /top   - Show the most common dependencies")
    print("  /nodes - Show details about nodes (sorted by downloads)")
    print("  /update - Fetch latest nodes from registry and update nodes.json")
    print("  /graph - Create cumulative dependencies visualization")
    print("  /summary - Show overall dependency analysis summary")
    print("  /help  - Show this help message")
    print("  /quit  - Exit interactive mode")
    print("\nSearch modifiers:")
    print("  * - Wildcard (e.g., torch*, *audio*)")
    print("  &save - Save results to file")
    print("  &all - Show all results without limits")
    print("  &top N - Only analyze top N nodes by downloads")
    print("         Use negative for bottom N (e.g., &top -10)")
    print("  &nodes - Filter by specific node IDs")
    print("         Comma-separated: &nodes id1,id2")
    print("         From file: &nodes file:nodelist.txt")
    print("  Combine: numpy &top 50 &save")
    print("\nOr type a dependency name directly (e.g., numpy, torch)")


def execute_command(nodes_dict, command):
    """
    Execute a single command from the command line.

    Args:
        nodes_dict: Dictionary of nodes
        command: Command to execute
    """
    # Mock stdin with our command followed by quit
    import io
    import sys

    # Save the original stdin
    original_stdin = sys.stdin

    # Create a mock stdin with our command and quit
    sys.stdin = io.StringIO(f"{command}\n/quit\n")

    try:
        # Run interactive mode with our mocked input
        interactive_mode(nodes_dict)
    finally:
        # Restore original stdin
        sys.stdin = original_stdin


def interactive_mode(nodes_dict):
    """
    Interactive chat loop for dependency queries.
    """
    print("\n" + "="*60)
    print("INTERACTIVE DEPENDENCY ANALYZER")
    print("="*60)
    print("\nEnter a dependency name to analyze (or use /quit to exit)")

    # Print help information
    print_help()

    # Pre-compile all dependencies for quick lookup
    dep_analysis = compile_dependencies(nodes_dict)
    # Use unique base dependencies instead of raw unique dependencies
    all_deps_lower = {dep.lower(): dep for dep in dep_analysis['unique_base_dependencies']}

    while True:
        try:
            query = input("\n> ").strip()

            # Handle exit commands
            if query.lower() in ['/quit', '/exit', '/q']:
                print("Exiting interactive mode.")
                break

            elif query.lower() == '/update':
                # Update nodes.json from the registry
                print("\nFetching latest nodes from registry...")
                try:
                    registry_data = get_registry_nodes(print_time=True)

                    # Save using the shared function
                    if save_nodes_json(registry_data):
                        # Reload the nodes dictionary with the updated data
                        nodes_dict = load_nodes_to_dict()

                        # Re-compile dependencies for the new data
                        dep_analysis = compile_dependencies(nodes_dict)
                        all_deps_lower = {dep.lower(): dep for dep in dep_analysis['unique_base_dependencies']}

                        print("Nodes data has been refreshed and is ready for analysis.")
                    else:
                        print("Failed to save nodes data")
                except Exception as e:
                    print(f"Error updating nodes.json: {e}")

            elif query.lower() == '/summary':
                # Display the default summary
                display_summary(nodes_dict)

            elif query.lower() == '/graph' or query.lower().startswith('/graph '):
                # Parse modifiers for /graph command
                save_results = False
                top_n = None
                working_nodes = nodes_dict
                original_query = query

                # Parse &nodes modifier first to filter by specific nodes
                working_nodes = parse_nodes_modifier(query, working_nodes)

                # Parse &save modifier
                if '&save' in query.lower():
                    save_results = True

                # Parse &top modifier if present
                if '&top' in query.lower():
                    import re
                    top_match = re.search(r'&top\s+(-?\d+)', query.lower())
                    if top_match:
                        top_n = int(top_match.group(1))
                        # Filter to top N nodes by downloads
                        sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)
                        if top_n > 0:
                            working_nodes = dict(sorted_nodes[:top_n])
                            print(f"\n[Filtering to top {top_n} nodes by downloads]")
                        else:
                            # Negative number means bottom N nodes
                            working_nodes = dict(sorted_nodes[top_n:])
                            print(f"\n[Filtering to bottom {abs(top_n)} nodes by downloads]")

                # Create cumulative dependencies graph with filtered nodes
                create_cumulative_graph(working_nodes, save_to_file=save_results, query_desc=original_query)

            elif query.lower() == '/help':
                print_help()

            elif query.lower() == '/list' or query.lower().startswith('/list '):
                # Check for modifiers in /list command
                working_nodes = nodes_dict
                top_n = None
                show_dupes = False
                save_results = False
                original_query = query

                # Parse &nodes modifier first to filter by specific nodes
                working_nodes = parse_nodes_modifier(query, working_nodes)

                # Parse &dupes modifier
                if '&dupes' in query.lower():
                    show_dupes = True

                # Parse &save modifier
                if '&save' in query.lower():
                    save_results = True

                # Parse &top modifier if present
                if '&top' in query.lower():
                    import re
                    top_match = re.search(r'&top\s+(-?\d+)', query.lower())
                    if top_match:
                        top_n = int(top_match.group(1))
                        # Filter to top N nodes by downloads
                        sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)
                        if top_n > 0:
                            working_nodes = dict(sorted_nodes[:top_n])
                            print(f"\n[Filtering to top {top_n} nodes by downloads]")
                        else:
                            # Negative number means bottom N nodes
                            working_nodes = dict(sorted_nodes[top_n:])
                            print(f"\n[Filtering to bottom {abs(top_n)} nodes by downloads]")

                # Re-compile dependencies for the filtered set
                filtered_dep_analysis = compile_dependencies(working_nodes)

                if show_dupes:
                    # Separate dependencies into duplicates and non-duplicates
                    dupes_only = []
                    non_dupes = []

                    for dep_name, versions in filtered_dep_analysis['dependency_versions'].items():
                        count = filtered_dep_analysis['dependency_count'].get(dep_name, 0)
                        if len(versions) > 1:
                            dupes_only.append((dep_name, count, versions))
                        else:
                            # Non-duplicates have only one version
                            non_dupes.append((dep_name, count, list(versions)[0] if versions else '*'))

                    # Sort by count (most used first), then alphabetically
                    dupes_only.sort(key=lambda x: (-x[1], x[0].lower()))
                    non_dupes.sort(key=lambda x: (-x[1], x[0].lower()))

                    output_lines = []

                    # Header for duplicates section
                    if top_n:
                        if top_n > 0:
                            output_lines.append(f"\nDEPENDENCIES WITH MULTIPLE VERSIONS in top {top_n} nodes ({len(dupes_only)} total):")
                        else:
                            output_lines.append(f"\nDEPENDENCIES WITH MULTIPLE VERSIONS in bottom {abs(top_n)} nodes ({len(dupes_only)} total):")
                    else:
                        output_lines.append(f"\nDEPENDENCIES WITH MULTIPLE VERSIONS ({len(dupes_only)} total):")

                    output_lines.append("="*60)

                    for dep_name, count, versions in dupes_only:
                        output_lines.append(f"\n{dep_name} ({count} nodes total, {len(versions)} different specs):")
                        # Sort versions by frequency
                        version_list = list(versions)
                        version_counts = []
                        for v in version_list:
                            # Count how many nodes use this specific version
                            v_count = sum(1 for node_id, node_data in working_nodes.items()
                                        if 'latest_version' in node_data and node_data['latest_version']
                                        and 'dependencies' in node_data['latest_version']
                                        and any(str(d).strip() == v for d in node_data['latest_version']['dependencies']))
                            version_counts.append((v, v_count))

                        version_counts.sort(key=lambda x: -x[1])
                        for v, v_count in version_counts:  # Show all versions
                            output_lines.append(f"  {v:40} ({v_count} nodes)")

                    # Add non-duplicates section
                    if non_dupes:
                        output_lines.append("")
                        if top_n:
                            if top_n > 0:
                                output_lines.append(f"\nDEPENDENCIES WITH SINGLE VERSION in top {top_n} nodes ({len(non_dupes)} total):")
                            else:
                                output_lines.append(f"\nDEPENDENCIES WITH SINGLE VERSION in bottom {abs(top_n)} nodes ({len(non_dupes)} total):")
                        else:
                            output_lines.append(f"\nDEPENDENCIES WITH SINGLE VERSION ({len(non_dupes)} total):")
                        output_lines.append("="*60)

                        for dep_name, count, version_spec in non_dupes:
                            output_lines.append(f"{dep_name:40} ({count:3} nodes) - {version_spec}")

                    # Print output
                    output_text = '\n'.join(output_lines)
                    print(output_text)

                    # Save if requested
                    if save_results:
                        save_results_to_file("list_dupes_" + original_query.replace('/list', '').strip(), output_text)

                else:
                    # Normal /list behavior
                    # Sort dependencies alphabetically but include counts
                    deps_with_counts = [(dep, filtered_dep_analysis['dependency_count'].get(dep, 0))
                                       for dep in filtered_dep_analysis['unique_base_dependencies']]
                    deps_with_counts.sort(key=lambda x: x[0].lower())

                    if top_n:
                        if top_n > 0:
                            print(f"\nUnique package names in top {top_n} nodes ({len(deps_with_counts)} total):")
                        else:
                            print(f"\nUnique package names in bottom {abs(top_n)} nodes ({len(deps_with_counts)} total):")
                    else:
                        print(f"\nAll unique package names ({len(deps_with_counts)} total):")
                    print("(Versions are grouped together under base package names)")
                    print("(Number in parentheses shows how many nodes use this dependency)")

                    # Print in columns for readability with counts
                    for i in range(0, len(deps_with_counts), 2):
                        row = deps_with_counts[i:i+2]
                        print("  " + " | ".join(f"{dep:30} ({count:3})" for dep, count in row))

                    # Save if requested
                    if save_results:
                        output_lines = []
                        if top_n:
                            if top_n > 0:
                                output_lines.append(f"Unique package names in top {top_n} nodes ({len(deps_with_counts)} total):")
                            else:
                                output_lines.append(f"Unique package names in bottom {abs(top_n)} nodes ({len(deps_with_counts)} total):")
                        else:
                            output_lines.append(f"All unique package names ({len(deps_with_counts)} total):")
                        output_lines.append("(Versions are grouped together under base package names)")
                        output_lines.append("(Number in parentheses shows how many nodes use this dependency)\n")

                        for dep, count in deps_with_counts:
                            output_lines.append(f"{dep:40} ({count:3})")

                        save_text = '\n'.join(output_lines)
                        save_results_to_file("list_" + original_query.replace('/list', '').strip(), save_text)

            elif query.lower() == '/top' or query.lower().startswith('/top '):
                # Check for &top modifier in /top command
                working_nodes = nodes_dict
                top_n = None

                # Parse &nodes modifier first to filter by specific nodes
                working_nodes = parse_nodes_modifier(query, working_nodes)

                # Parse &top modifier if present
                if '&top' in query.lower():
                    import re
                    top_match = re.search(r'&top\s+(-?\d+)', query.lower())
                    if top_match:
                        top_n = int(top_match.group(1))
                        # Filter to top N nodes by downloads
                        sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)
                        if top_n > 0:
                            working_nodes = dict(sorted_nodes[:top_n])
                            print(f"\n[Filtering to top {top_n} nodes by downloads]")
                        else:
                            # Negative number means bottom N nodes
                            working_nodes = dict(sorted_nodes[top_n:])
                            print(f"\n[Filtering to bottom {abs(top_n)} nodes by downloads]")

                # Re-compile dependencies for the filtered set
                filtered_dep_analysis = compile_dependencies(working_nodes)

                if top_n:
                    if top_n > 0:
                        print(f"\nTop 20 most common dependencies in top {top_n} nodes:")
                    else:
                        print(f"\nTop 20 most common dependencies in bottom {abs(top_n)} nodes:")
                else:
                    print("\nTop 20 most common dependencies:")
                for dep, count in filtered_dep_analysis['sorted_by_frequency'][:20]:
                    print(f"  {dep:30} - {count} nodes")

            elif query.lower() == '/nodes' or query.lower().startswith('/nodes '):
                # Parse modifiers for /nodes command
                save_results = False
                show_all = False
                top_n = None
                working_nodes = nodes_dict
                original_query = query

                # Parse &nodes modifier first to filter by specific nodes
                working_nodes = parse_nodes_modifier(query, working_nodes)

                # Parse modifiers
                if '&save' in query.lower():
                    save_results = True

                if '&all' in query.lower():
                    show_all = True

                if '&top' in query.lower():
                    import re
                    top_match = re.search(r'&top\s+(-?\d+)', query.lower())
                    if top_match:
                        top_n = int(top_match.group(1))
                        # Filter to top N nodes by downloads
                        sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)
                        if top_n > 0:
                            working_nodes = dict(sorted_nodes[:top_n])
                            print(f"\n[Filtering to top {top_n} nodes by downloads]")
                        else:
                            # Negative number means bottom N nodes
                            working_nodes = dict(sorted_nodes[top_n:])
                            print(f"\n[Filtering to bottom {abs(top_n)} nodes by downloads]")

                # Calculate ranks for ALL nodes (not just working_nodes)
                full_rank_map = calculate_node_ranks(nodes_dict)

                # Sort all nodes by downloads
                sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)

                # Determine how many to show
                nodes_to_display = sorted_nodes if show_all else sorted_nodes[:20]

                # Build output
                output_lines = []
                if top_n:
                    if top_n > 0:
                        output_lines.append(f"\nTop {min(len(nodes_to_display), top_n)} nodes by downloads:")
                    else:
                        output_lines.append(f"\nBottom {min(len(nodes_to_display), abs(top_n))} nodes by downloads:")
                elif show_all:
                    output_lines.append(f"\nAll {len(nodes_to_display)} nodes by downloads:")
                else:
                    output_lines.append(f"\nTop {min(len(nodes_to_display), 20)} nodes by downloads:")

                output_lines.append("="*60)

                for i, (node_id, node_data) in enumerate(nodes_to_display, 1):
                    # Extract node information
                    name = node_data.get('name', 'N/A')
                    downloads = node_data.get('downloads', 0)
                    stars = node_data.get('github_stars', 0)
                    repo = node_data.get('repository', 'N/A')
                    description = node_data.get('description', 'N/A')

                    # Get latest version info
                    latest_version_info = node_data.get('latest_version', {})
                    latest_date = 'N/A'
                    version = 'N/A'
                    dep_count = 0

                    if latest_version_info:
                        if 'createdAt' in latest_version_info:
                            date_str = latest_version_info['createdAt']
                            latest_date = date_str[:10] if date_str else 'N/A'

                        version = latest_version_info.get('version', 'N/A')

                        if 'dependencies' in latest_version_info:
                            deps = latest_version_info['dependencies']
                            if deps and isinstance(deps, list):
                                # Count only active dependencies (not commented or pip commands)
                                dep_count = sum(1 for d in deps if not str(d).strip().startswith('#') and not str(d).strip().startswith('--'))

                    # Format output
                    rank = full_rank_map.get(node_id, 'N/A')
                    output_lines.append(f"\n{i}. {name} ({node_id})")
                    output_lines.append(f"   Rank: #{rank} | Downloads: {downloads:,} | Stars: {stars:,} | Dependencies: {dep_count}")
                    output_lines.append(f"   Latest: {latest_date} | Version: {version}")
                    if len(description) > 100:
                        output_lines.append(f"   Description: {description[:100]}...")
                    else:
                        output_lines.append(f"   Description: {description}")
                    output_lines.append(f"   Repository: {repo}")

                if not show_all and len(sorted_nodes) > 20:
                    output_lines.append(f"\n... and {len(sorted_nodes) - 20} more nodes")
                    output_lines.append("Use &all to see all nodes")

                # Print output
                output_text = '\n'.join(output_lines)
                print(output_text)

                # Save if requested
                if save_results:
                    # For saved file, always show all nodes in the working set
                    save_lines = []
                    if top_n:
                        if top_n > 0:
                            save_lines.append(f"All {len(sorted_nodes)} nodes (filtered to top {top_n} by downloads):")
                        else:
                            save_lines.append(f"All {len(sorted_nodes)} nodes (filtered to bottom {abs(top_n)} by downloads):")
                    else:
                        save_lines.append(f"All {len(sorted_nodes)} nodes by downloads:")
                    save_lines.append("="*60)

                    for i, (node_id, node_data) in enumerate(sorted_nodes, 1):
                        name = node_data.get('name', 'N/A')
                        downloads = node_data.get('downloads', 0)
                        stars = node_data.get('github_stars', 0)
                        repo = node_data.get('repository', 'N/A')
                        description = node_data.get('description', 'N/A')

                        latest_version_info = node_data.get('latest_version', {})
                        latest_date = 'N/A'
                        version = 'N/A'
                        dep_count = 0

                        if latest_version_info:
                            if 'createdAt' in latest_version_info:
                                date_str = latest_version_info['createdAt']
                                latest_date = date_str[:10] if date_str else 'N/A'

                            version = latest_version_info.get('version', 'N/A')

                            if 'dependencies' in latest_version_info:
                                deps = latest_version_info['dependencies']
                                if deps and isinstance(deps, list):
                                    dep_count = sum(1 for d in deps if not str(d).strip().startswith('#') and not str(d).strip().startswith('--'))

                        rank = full_rank_map.get(node_id, 'N/A')
                        save_lines.append(f"\n{i}. {name} ({node_id})")
                        save_lines.append(f"   Rank: #{rank} | Downloads: {downloads:,} | Stars: {stars:,} | Dependencies: {dep_count}")
                        save_lines.append(f"   Latest: {latest_date} | Version: {version}")
                        save_lines.append(f"   Description: {description}")
                        save_lines.append(f"   Repository: {repo}")

                    save_text = '\n'.join(save_lines)
                    save_results_to_file("nodes_" + original_query.replace('/nodes', '').strip(), save_text)

            elif query:
                # Check for modifiers
                save_results = False
                show_all = False
                top_n = None
                working_nodes = nodes_dict
                original_query = query

                # Parse modifiers (case-insensitive)
                query_lower = query.lower()

                # Parse &nodes modifier first and remove it from query
                if '&nodes' in query_lower:
                    working_nodes = parse_nodes_modifier(original_query, working_nodes)
                    # Remove &nodes specification from query
                    import re
                    query = re.sub(r'&nodes\s+[^&]+', '', query, flags=re.IGNORECASE).strip()
                    query_lower = query.lower()

                # Parse &top N modifier
                if '&top' in query_lower:
                    import re
                    top_match = re.search(r'&top\s+(-?\d+)', query_lower)
                    if top_match:
                        top_n = int(top_match.group(1))
                        # Remove the &top N from the query
                        query = re.sub(r'&top\s+-?\d+', '', query, flags=re.IGNORECASE).strip()
                        query_lower = query.lower()

                        # Filter to top N nodes by downloads
                        sorted_nodes = sorted(working_nodes.items(), key=lambda x: x[1].get('downloads', 0), reverse=True)
                        if top_n > 0:
                            working_nodes = dict(sorted_nodes[:top_n])
                            print(f"\n[Filtering to top {top_n} nodes by downloads]")
                        else:
                            # Negative number means bottom N nodes
                            working_nodes = dict(sorted_nodes[top_n:])
                            print(f"\n[Filtering to bottom {abs(top_n)} nodes by downloads]")

                if '&save' in query_lower:
                    save_results = True
                    query = query.replace('&save', '').replace('&SAVE', '').replace('&Save', '').strip()
                    query_lower = query.lower()

                if '&all' in query_lower:
                    show_all = True
                    query = query.replace('&all', '').replace('&ALL', '').replace('&All', '').strip()

                # Capture output for saving if needed
                output_lines = []

                # Re-compile dependencies for the filtered set if using &top
                if top_n:
                    dep_analysis = compile_dependencies(working_nodes)
                    all_deps_lower = {dep.lower(): dep for dep in dep_analysis['unique_base_dependencies']}

                # Check if query contains wildcard
                if '*' in query:
                    print(f"\nSearching for dependencies matching pattern: {query}")
                    wildcard_results = analyze_wildcard_dependencies(working_nodes, query)

                    if wildcard_results:
                        # Sort by total nodes using each dependency
                        sorted_results = sorted(wildcard_results.items(),
                                              key=lambda x: x[1]['total_nodes'],
                                              reverse=True)

                        # Build output for display and saving
                        output_lines = []
                        output_lines.append(f"\nFound {len(wildcard_results)} dependencies matching '{query}':")
                        output_lines.append("="*60)

                        total_nodes = sum(info['total_nodes'] for _, info in wildcard_results.items())
                        output_lines.append(f"Total nodes using any matching dependency: {total_nodes}")

                        # Format each dependency's details
                        for dep_name, dep_info in sorted_results:
                            # Use show_all flag for display
                            if show_all:
                                output_lines.append(format_dependency_details(dep_info, show_all_nodes=True))
                            else:
                                # Create limited version for display
                                output_lines.append(f"\n{'='*50}")
                                output_lines.append(f"Dependency: {dep_name}")
                                output_lines.append(f"{'='*50}")
                                output_lines.append(f"Total nodes using this dependency: {dep_info['total_nodes']}")

                                if dep_info['commented_count'] > 0:
                                    output_lines.append(f"Nodes with commented-out lines: {dep_info['commented_count']}")

                                if dep_info['total_nodes'] > 0:
                                    # Show version distribution
                                    if len(dep_info['sorted_versions']) > 0:
                                        output_lines.append(f"\nVersion specifications found:")
                                        # Show up to 5 versions for wildcard results
                                        for version, count in dep_info['sorted_versions'][:5]:
                                            output_lines.append(f"  {version:20} - {count} nodes")
                                        if len(dep_info['sorted_versions']) > 5:
                                            output_lines.append(f"  ... and {len(dep_info['sorted_versions']) - 5} more versions")

                                    # Sort nodes by downloads (most popular first)
                                    sorted_nodes = sorted(dep_info['nodes_using'],
                                                        key=lambda x: x.get('downloads', 0),
                                                        reverse=True)

                                    # Show top 5 nodes for wildcard results
                                    output_lines.append(f"\nTop nodes using {dep_name} (showing up to 5 by popularity):")
                                    for i, node in enumerate(sorted_nodes[:5], 1):
                                        output_lines.append(f"\n  {i}. {node['node_name']} ({node['node_id']})")
                                        output_lines.append(f"     Rank: #{node.get('rank', 'N/A')} | Downloads: {node.get('downloads', 0):,} | Stars: {node.get('stars', 0):,} | Latest: {node.get('latest_version_date', 'N/A')}")
                                        output_lines.append(f"     Spec: {node['dependency_spec']}")
                                        output_lines.append(f"     Repo: {node['repository']}")

                                    if dep_info['total_nodes'] > 5:
                                        output_lines.append(f"\n  ... and {dep_info['total_nodes'] - 5} more nodes using {dep_name}")

                                if dep_info['commented_count'] > 0 and dep_info['nodes_with_commented']:
                                    output_lines.append(f"\n  Nodes with commented-out {dep_name} lines:")
                                    for i, node in enumerate(dep_info['nodes_with_commented'][:2], 1):
                                        output_lines.append(f"    {i}. {node['node_name']} ({node['node_id']})")
                                    if dep_info['commented_count'] > 2:
                                        output_lines.append(f"    ... and {dep_info['commented_count'] - 2} more with commented {dep_name}")

                        output_lines.append("\n" + "="*60)
                        output_lines.append("To see details for a specific dependency, type its full name.")

                        # Print output
                        output_text = '\n'.join(output_lines)
                        print(output_text)

                        # Save if requested
                        if save_results:
                            # For saved file, regenerate with all nodes shown
                            save_lines = []
                            save_lines.append(f"Found {len(wildcard_results)} dependencies matching '{query}':")
                            save_lines.append("="*60)
                            save_lines.append(f"Total nodes using any matching dependency: {total_nodes}")

                            for dep_name, dep_info in sorted_results:
                                save_lines.append(format_dependency_details(dep_info, show_all_nodes=True))

                            save_text = '\n'.join(save_lines)
                            save_results_to_file(original_query, save_text)
                    else:
                        print(f"\nNo dependencies found matching pattern '{query}'")

                # Regular non-wildcard query
                else:
                    # Look for exact match first (case-insensitive)
                    query_lower = query.lower()
                    exact_match = None

                    # Check for exact match
                    if query_lower in all_deps_lower:
                        exact_match = all_deps_lower[query_lower]

                    if exact_match:
                        result = analyze_specific_dependency(working_nodes, exact_match)

                        # Format output for display (use show_all flag)
                        output_text = format_dependency_details(result, show_all_nodes=show_all)
                        print(output_text)

                        # Save if requested
                        if save_results:
                            # Always save with all nodes shown
                            full_output = format_dependency_details(result, show_all_nodes=True)
                            save_results_to_file(original_query, full_output)
                    else:
                        # Try to find dependencies starting with the search string
                        starts_with_matches = [dep for dep_lower, dep in all_deps_lower.items()
                                             if dep_lower.startswith(query_lower)]

                        if starts_with_matches:
                            print(f"\nNo exact match for '{query}'. Found {len(starts_with_matches)} dependencies starting with '{query}':\n")

                            # Sort by frequency (most used first)
                            starts_with_sorted = sorted(starts_with_matches,
                                                      key=lambda x: dep_analysis['dependency_count'].get(x, 0),
                                                      reverse=True)

                            # Show all matches if 20 or fewer, otherwise show top 20
                            show_limit = len(starts_with_sorted) if len(starts_with_sorted) <= 20 else 20

                            for match in starts_with_sorted[:show_limit]:
                                count = dep_analysis['dependency_count'].get(match, 0)
                                print(f"  - {match:40} ({count} nodes)")

                            if len(starts_with_sorted) > show_limit:
                                print(f"\n  ... and {len(starts_with_sorted) - show_limit} more dependencies starting with '{query}'")
                                print(f"  (Showing top {show_limit} by usage)")

                            print(f"\nTo analyze any of these, type the full dependency name.")
                        else:
                            # If no starts-with matches, try substring match anywhere
                            partial_matches = [dep for dep_lower, dep in all_deps_lower.items()
                                             if query_lower in dep_lower]

                            if partial_matches:
                                print(f"\nNo dependencies starting with '{query}'. Found {len(partial_matches)} containing '{query}':")

                                # Sort by frequency
                                partial_sorted = sorted(partial_matches[:20],
                                                      key=lambda x: dep_analysis['dependency_count'].get(x, 0),
                                                      reverse=True)

                                for match in partial_sorted[:10]:
                                    count = dep_analysis['dependency_count'].get(match, 0)
                                    print(f"  - {match:40} ({count} nodes)")
                                if len(partial_matches) > 10:
                                    print(f"  ... and {len(partial_matches) - 10} more matches containing '{query}'")
                            else:
                                print(f"\nNo dependency found matching '{query}'")
                                print("Use 'list' to see all available dependencies")

        except KeyboardInterrupt:
            print("\n\nInterrupted. Exiting interactive mode.")
            break
        except Exception as e:
            print(f"Error: {e}")


def display_summary(nodes_dict):
    """
    Display the summary of nodes and dependencies.
    This is the default output when not using --ask.
    """
    print(f"\nFirst 3 node IDs in the dictionary:")
    for i, node_id in enumerate(list(nodes_dict.keys())[:3]):
        print(f"  - {node_id}")

    print(f"\nExample node data for first ID:")
    first_id = list(nodes_dict.keys())[0]
    first_node = nodes_dict[first_id]
    print(f"ID: {first_id}")
    print(f"Name: {first_node.get('name', 'N/A')}")
    print(f"Repository: {first_node.get('repository', 'N/A')}")
    print(f"Description: {first_node.get('description', 'N/A')[:100]}...")

    print("\n" + "="*60)
    print("DEPENDENCY ANALYSIS")
    print("="*60)

    dep_analysis = compile_dependencies(nodes_dict)

    print(f"\nTotal nodes analyzed: {len(nodes_dict)}")
    print(f"Nodes with active dependencies: {dep_analysis['nodes_with_deps_count']}")
    print(f"Nodes without active dependencies: {dep_analysis['nodes_without_deps_count']}")
    if dep_analysis['nodes_with_commented_count'] > 0:
        print(f"Nodes with commented-out lines: {dep_analysis['nodes_with_commented_count']}")
    if dep_analysis['nodes_with_pip_commands_count'] > 0:
        print(f"INFO: Nodes with pip commands (--): {dep_analysis['nodes_with_pip_commands_count']}")
    if dep_analysis['nodes_with_git_deps_count'] > 0:
        print(f"Nodes with git-based dependencies: {dep_analysis['nodes_with_git_deps_count']}")
    print(f"\nTotal active dependency references: {dep_analysis['total_dependencies']}")
    print(f"Unique active packages (grouping versions): {dep_analysis['unique_count']}")
    print(f"Unique dependency specifications: {dep_analysis['unique_raw_count']}")
    if len(dep_analysis['unique_commented_dependencies']) > 0:
        print(f"Unique commented dependencies: {len(dep_analysis['unique_commented_dependencies'])}")

    print(f"\nTop 10 most common dependencies:")
    for dep, count in dep_analysis['sorted_by_frequency'][:10]:
        print(f"  - {dep}: {count} nodes")

    if dep_analysis['nodes_with_commented_count'] > 0:
        print(f"\n\nNOTE: {dep_analysis['nodes_with_commented_count']} nodes have commented-out lines")
        print("  Lines starting with # are comments and are not active dependencies")
        print(f"\n  Example nodes with commented lines:")
        for node_info in dep_analysis['nodes_with_commented_dependencies'][:3]:
            print(f"\n    Node: {node_info['name']} ({node_info['id']})")
            print(f"    Commented deps: {', '.join(node_info['commented_deps'][:3])}")
            if len(node_info['commented_deps']) > 3:
                print(f"      ... and {len(node_info['commented_deps']) - 3} more commented")

    if dep_analysis['nodes_with_pip_commands_count'] > 0:
        print(f"\n\nINFO: {dep_analysis['nodes_with_pip_commands_count']} nodes use pip command flags")
        print("  These are special pip installation flags (starting with --)")
        print("  Common pip commands found:")
        for cmd, count in dep_analysis['sorted_pip_commands'][:5]:
            print(f"    {cmd}: {count} nodes")
        if len(dep_analysis['sorted_pip_commands']) > 5:
            print(f"    ... and {len(dep_analysis['sorted_pip_commands']) - 5} more unique pip commands")

        print(f"\n  Example nodes with pip commands:")
        for node_info in dep_analysis['nodes_with_pip_commands'][:3]:
            print(f"\n    Node: {node_info['name']} ({node_info['id']})")
            print(f"    Pip commands: {', '.join(node_info['pip_commands'][:2])}")
            if len(node_info['pip_commands']) > 2:
                print(f"      ... and {len(node_info['pip_commands']) - 2} more commands")

    if dep_analysis['nodes_with_git_deps_count'] > 0:
        print(f"\n\nGit-based Dependencies: {dep_analysis['nodes_with_git_deps_count']} nodes")
        print("  These dependencies are installed directly from git repositories")

        if dep_analysis['sorted_git_dependency_types']:
            print("  Breakdown by type:")
            for dep_type, count in dep_analysis['sorted_git_dependency_types']:
                print(f"    {dep_type}: {count} dependencies")

        print(f"  Total unique git dependencies: {len(dep_analysis['unique_git_dependencies'])}")

        print(f"\n  Example nodes with git dependencies:")
        for node_info in dep_analysis['nodes_with_git_dependencies'][:3]:
            print(f"\n    Node: {node_info['name']} ({node_info['id']})")
            for git_dep in node_info['git_deps'][:2]:
                print(f"    Git dep: {git_dep}")
            if len(node_info['git_deps']) > 2:
                print(f"      ... and {len(node_info['git_deps']) - 2} more git dependencies")


def main():
    parser = argparse.ArgumentParser(description='Analyze ComfyUI node dependencies')
    parser.add_argument('--execute', '-e', type=str,
                       help='Execute a command as if in interactive mode (e.g., --execute "/summary")')
    args = parser.parse_args()

    # Special handling for /update command when nodes.json doesn't exist
    if args.execute and args.execute.lower() in ['/update', '//update']:
        # Try to run update directly without loading nodes first
        print("\nFetching latest nodes from registry...")
        try:
            registry_data = get_registry_nodes(print_time=True)

            # Save using the shared function
            if not save_nodes_json(registry_data):
                print("Failed to save nodes data")
            return
        except Exception as e:
            print(f"Error updating nodes.json: {e}")
            return

    nodes_dict = load_nodes_to_dict()

    if not nodes_dict:
        print("nodes.json not found. Fetching from registry...")
        try:
            registry_data = get_registry_nodes(print_time=True)

            # Save using the shared function
            if save_nodes_json(registry_data):
                # Reload the nodes dictionary
                nodes_dict = load_nodes_to_dict()

                if not nodes_dict:
                    print("Error: Failed to load nodes data even after update")
                    return
            else:
                print("Failed to save nodes data")
                return
        except Exception as e:
            print(f"Error fetching nodes from registry: {e}")
            return

    if args.execute:
        # Execute the command in a modified interactive mode
        execute_command(nodes_dict, args.execute)
    else:
        # Default behavior - interactive mode
        interactive_mode(nodes_dict)


if __name__ == "__main__":
    main()